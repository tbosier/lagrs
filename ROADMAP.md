# Lagrs Development Roadmap

## Current Status âœ…

- âœ… Parallel rolling mean (Rayon)
- âœ… ARIMA(p,d,q) with MLE estimation
- âœ… Parallel batch processing for multi-SKU
- âœ… Zero-copy Python-Rust data transfer
- âœ… Comprehensive test suite
- âœ… GitHub Actions CI/CD

## Roadmap Analysis & Recommendations

### ğŸ¯ **Phase 1: Fast Rolling Ops** (RECOMMENDED - Quick Win)

**Status**: Partially done (rolling mean exists)

**What to add:**
- âœ… Rolling mean (done)
- â³ Rolling std
- â³ Rolling sum
- â³ Windowed min/max
- â³ Rolling median (more complex, but useful)

**Timeline**: 2-3 days (not 3-7)

**Why this is good:**
- âœ… Quick wins build momentum
- âœ… Easy to benchmark and show value
- âœ… Foundation for other features
- âœ… Low risk, high visibility

**Recommendation**: **DO THIS FIRST** - It's almost done, just needs a few more functions.

---

### âš™ï¸ **Phase 2: State Space Foundation** (CONSIDER DELAYING)

**Timeline**: 2-4 weeks (realistic)

**Issues:**
- âŒ **Too early** - You don't need this for ARIMA
- âŒ **Over-engineering** - ARIMA can work without full state-space
- âŒ **Complexity** - Kalman filter is non-trivial
- âœ… **Future-proof** - But not urgent

**Recommendation**: **SKIP FOR NOW** - Come back after Phase 3 if you need SARIMAX/VARMAX. Current ARIMA implementation works fine without it.

**Better alternative**: Focus on making ARIMA robust first, then add state-space later if needed.

---

### ğŸ¯ **Phase 3: ARIMA v0.1 Complete** (IN PROGRESS - PRIORITIZE)

**Status**: 70% done

**What's done:**
- âœ… ARIMA(p,d,q) basic implementation
- âœ… MLE estimation (gradient descent)
- âœ… Differencing
- âœ… Forecasting
- âœ… AIC/BIC

**What's missing:**
- â³ SARIMA(P,D,Q,s) - seasonal ARIMA
- â³ Better optimization (BFGS instead of gradient descent)
- â³ Parameter constraints (stationarity/invertibility)
- â³ Confidence intervals for forecasts
- â³ Model diagnostics (residual analysis)

**Timeline**: 1-2 weeks (not 2-3)

**Recommendation**: **COMPLETE THIS** - It's your core differentiator. Add:
1. SARIMA support (high value)
2. Better optimization (accuracy)
3. Confidence intervals (production-ready)

---

### ğŸ§  **Phase 4: Auto Model Selection** (HIGH VALUE)

**Timeline**: 1-2 weeks (realistic)

**Why this matters:**
- âœ… **Huge user value** - Everyone wants auto-arima
- âœ… **Differentiator** - pmdarima is slow
- âœ… **Parallelizable** - Perfect for Rust

**Recommendation**: **DO THIS** - But simplify:
- Start with simple grid search (p,d,q only)
- Add seasonal later
- Use parallel evaluation (Rayon)
- Cache results

**Success metric**: 10-50x faster than pmdarima (achievable with parallelization)

---

### ğŸŒ™ **Phase 5: ETS / Holt-Winters** (MEDIUM PRIORITY)

**Timeline**: 1-2 weeks (realistic)

**Why it's good:**
- âœ… Popular for retail/forecasting
- âœ… Simpler than ARIMA
- âœ… Good complement

**Recommendation**: **DO THIS** - But after Phase 4. ETS is simpler and faster to implement.

---

### ğŸ”® **Phase 6: Seasonality Detection** (MEDIUM PRIORITY)

**Timeline**: 1-2 weeks (realistic)

**Why it matters:**
- âœ… Enables better auto-selection
- âœ… Useful for Prophet-like features
- âœ… STL is well-understood

**Recommendation**: **CONSIDER** - But can be done incrementally. Start with simple periodogram detection, add STL later.

---

### ğŸ’¥ **Phase 7: Gradient-Boosted TS** (LOW PRIORITY - TOO AMBITIOUS)

**Timeline**: 3-6 weeks (probably 8-12 weeks realistically)

**Issues:**
- âŒ **Huge scope** - This is basically building XGBoost
- âŒ **Competition** - XGBoost, LightGBM already exist
- âŒ **Different problem** - Not time-series specific
- âŒ **Complexity** - Tree algorithms are non-trivial

**Recommendation**: **SKIP OR DELAY** - Focus on time-series specific models first. Users can use XGBoost with lag features if needed.

**Alternative**: Build a **feature engineering** library that generates lag/rolling features for XGBoost instead.

---

### ğŸ” **Phase 8: Cross-Validation** (HIGH VALUE)

**Timeline**: 1 week (realistic)

**Why it matters:**
- âœ… **Essential for production** - Everyone needs backtesting
- âœ… **Easy to parallelize** - Perfect for Rust
- âœ… **Differentiator** - Fast backtesting is valuable

**Recommendation**: **DO THIS EARLY** - It's relatively simple and high value. Can be done after Phase 3.

---

### ğŸ—ï¸ **Phase 9: Hierarchical Forecasting** (MEDIUM PRIORITY)

**Timeline**: 2-4 weeks (realistic)

**Why it matters:**
- âœ… **Enterprise need** - Many companies need this
- âœ… **Niche** - Less competition
- âœ… **Scalable** - Rust can handle large hierarchies

**Recommendation**: **CONSIDER** - But after core models are solid. This is advanced.

---

### â˜€ï¸ **Phase 10: Production Features** (ONGOING)

**Timeline**: Continuous

**What's needed:**
- âœ… Model serialization (save/load)
- âœ… Arrow/Parquet I/O
- âœ… Better error handling
- âœ… Logging
- âœ… Documentation

**Recommendation**: **DO INCREMENTALLY** - Add these as you go, not as a separate phase.

---

### ğŸ§Š **Phase 11: GPU Acceleration** (OPTIONAL - LATER)

**Timeline**: 4-8 weeks (probably longer)

**Issues:**
- âŒ **Huge investment** - CUDA is complex
- âŒ **Limited benefit** - Most users don't have GPUs
- âŒ **Maintenance burden** - GPU code is hard to maintain

**Recommendation**: **SKIP FOR NOW** - Focus on CPU parallelism first. GPU can come much later if there's demand.

---

### ğŸ§© **Phase 12: Documentation** (ONGOING)

**Recommendation**: **DO CONTINUOUSLY** - Don't wait. Document as you build.

---

## Revised Recommended Roadmap

### **Immediate (Next 2-4 weeks)**
1. âœ… **Complete Phase 1** - Add rolling std, sum, min/max (2-3 days)
2. âœ… **Complete Phase 3** - SARIMA, better optimization, confidence intervals (1-2 weeks)
3. âœ… **Phase 8** - Cross-validation/backtesting (1 week)

### **Short-term (1-2 months)**
4. âœ… **Phase 4** - Auto model selection (1-2 weeks)
5. âœ… **Phase 5** - ETS models (1-2 weeks)
6. âœ… **Phase 10** - Production features (ongoing)

### **Medium-term (3-6 months)**
7. â³ **Phase 6** - Seasonality detection (1-2 weeks)
8. â³ **Phase 9** - Hierarchical forecasting (2-4 weeks, if needed)

### **Later (If needed)**
9. â³ **Phase 2** - State-space foundation (only if SARIMAX/VARMAX needed)
10. â³ **Phase 7** - Gradient boosting (probably skip, use XGBoost instead)
11. â³ **Phase 11** - GPU (much later, if at all)

## Key Principles

1. **Focus on time-series specific features** - Don't rebuild general ML
2. **Leverage parallelism** - That's your advantage
3. **Incremental value** - Each phase should deliver usable features
4. **Realistic timelines** - ChatGPT's estimates are optimistic
5. **User feedback** - Build what users actually need

## What Makes lagrs Unique

1. **Speed** - Rust + parallelism = 10-100x faster
2. **Multi-SKU** - Parallel batch processing (unique!)
3. **Zero-copy** - Efficient Python integration
4. **Production-ready** - Fast enough for real-time use

Focus on these strengths rather than trying to build everything.

